def type(main)
    import architecture
    from framework.toolbox.laboratory import Laboratory
    from framework.toolbox.logger import Logger
    from framework.toolbox.stopper import EarlyStopper
    from implementations.$library.toolbox.experiment import $library+FirstUppercase~Experiment as Experiment
    $optimizer+import
    $loss+import
    [$dataset+import]
    from implementations.$library.toolbox.saver import $library+FirstUppercase~ModelSaver as ModelSaver
    $strategy+import

    dataset = $dataset

    experiments = $[$experiment...[, $NL$TAB$TAB$TAB$TAB]$]

    $laboratory

    print("The Lab loss is {}".format(loss))
end

def type(strategy) trigger(import)
    from implementations.$library.toolbox.strategies.$name+Lowercase import $library+FirstUppercase$name+FirstUppercase~Strategy as $name+FirstUppercase~Strategy
end

def type(optimizer) trigger(import)
    from implementations.$library.toolbox.optimizers.$name+Lowercase import $library+FirstUppercase$name~Optimizer as $name~Optimizer
end

def type(loss) trigger(import)
    from implementations.$library.toolbox.losses.$name+Lowercase import $library+FirstUppercase$name~LossFunction as $name~LossFunction
end

def type(dataset) trigger(import)
    from implementations.$library.toolbox.loaders.$name+Lowercase import $library+FirstUppercaseDatasetGenerator as DatasetGenerator
end

def type(laboratory)
    loss = Laboratory(name="$laboratoryName",
                       epochs=$epochs,
                       dataset=dataset,
                       architecture=architecture.architecture,
                       experiments=experiments,
                       strategy=$strategy,
                       logger=Logger("$path")).explore()
end

def type(strategy)
    $name+FirstUppercase~Strategy($loss)
end

def type(experiment)
    Experiment(name="$experimentName",
                              optimizer=$optimizer,
                              loss_function=$loss,
                              stopper=EarlyStopper($epochs, $patience),
                              saver=ModelSaver("$path"))
end

def type(loss)
    $name~LossFunction()
end

def type(dataset)
    $name+FirstUppercase~DatasetGenerator(name="$datasetName",
                                path=PATH,
                                batch_size=$batchSize,
                                random_state=$seed)
        .load(train_proportion=$trainProportion, validation_proportion=$valProportion, test_proportion=$testProportion)
end

def type(stopper)
    EarlyStopper($epochs, $patiente)
end





def type(optimizer) type(SGD)
    SGDOptimizer(parameters=architecture.architecture.parameters(), learning_rate=$lr, $momentum, $dampening, weight_decay=$weight_decay)
end

def type(optimizer) type(Adadelta)
    AdadeltaOptimizer(parameters=architecture.architecture.parameters(), learning_rate=$lr, rho=$rho, eps=$eps, weight_decay=$weight_decay)
end

def type(optimizer) type(Adagrad)
    AdagradOptimizer(parameters=architecture.architecture.parameters(), learning_rate=$lr, learning_rate_decay=$lr_decay, eps=$eps, weight_decay=$weight_decay)
end

def type(optimizer) type(Adam)
    AdamOptimizerOptimizer(parameters=architecture.architecture.parameters(), learning_rate=$lr, betas=($b0, $b1), eps=$eps, weight_decay=$weight_decay)
end

def type(optimizer) type(Adamax)
    AdamaxOptimizer(parameters=architecture.architecture.parameters(), learning_rate=$lr, betas=($b0, $b1), eps=$eps, weight_decay=$weight_decay)
end

def type(optimizer) type(AdamW)
    AdamWOptimizer(parameters=architecture.architecture.parameters(), learning_rate=$lr, betas=($b0, $b1), eps=$eps, weight_decay=$weight_decay)
end

def type(optimizer) type(AMSGrad)
    AMSGradOptimizer(parameters=architecture.architecture.parameters(), learning_rate=$lr, betas=($b0, $b1), eps=$eps, weight_decay=$weight_decay)
end

def type(optimizer) type(ASGD)
    ASGDOptimizer(parameters=architecture.architecture.parameters(), learning_rate=$lr, lambd=$lambd, alpha=$alpha, t0=$t0, weight_decay=$weight_decay)
end

def type(optimizer) type(NAdam)
    NAdamOptimizer(parameters=architecture.architecture.parameters(),learning_rate=$lr, betas=($b0, $b1), eps=$eps, weight_decay=$weight_decay)
end

def type(optimizer) type(RProp)
    RPropOptimizer(parameters=architecture.architecture.parameters(), learning_rate=$lr, etas=($eta0, $eta1), step_sizes=($step0, $step1))
end

def type(optimizer) type(RAdam)
    RAdamOptimizer(parameters=architecture.architecture.parameters(), learning_rate=$lr, ($b0, $b1), eps=$eps, weight_decay=$weight_decay)
end

def type(optimizer) type(RMSProp)
    RMSPropOptimizer(parameters=architecture.architecture.parameters(), learning_rate=$lr, alpha=$alpha, eps=$eps, weight_decay=$weight_decay, $momentum)
end

def type(optimizer) type(SparseAdam)
    SparseAdamOptimizer(parameters=architecture.architecture.parameters(), learning_rate=$lr, betas=($b0, $b1), eps=$eps)
end

def type(optimizer) type(LBFGS)
    LBFGSOptimizer(parameters=architecture.architecture.parameters(), learning_rate=$lr, rho=$rho, eps=$eps, weight_decay=$weight_decay)
end